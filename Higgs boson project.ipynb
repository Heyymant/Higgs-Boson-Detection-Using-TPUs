{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Searching for the Higgs Boson #\n\nThe Standard Model is a theory in particle physics that describes some of the most basic forces of nature. One fundamental particle, the Higgs boson, is what accounts for the *mass* of matter. First theorized in the 1964, the Higgs boson eluded observation for almost fifty years. In 2012 it was finally observed experimentally at the Large Hadron Collider. These experiments produced millions of gigabytes of data.\n\nLarge and complicated datasets like these are where deep learning excels. In this notebook, we'll build a Wide and Deep neural network to determine whether an observed particle collision produced a Higgs boson or not.\n\n# The Collision Data #\n\nThe collision of protons at high energy can produce new particles like the Higgs boson. These particles can't be directly observed, however, since they decay almost instantly. So to detect the presence of a new particle, we instead observe the behavior of the particles they decay into, their \"decay products\".\n\nThe *Higgs* dataset contains 21 \"low-level\" features of the decay products and also 7 more \"high-level\" features derived from these.\n\n# Wide and Deep Neural Networks #\n\nA *Wide and Deep* network trains a linear layer side-by-side with a deep stack of dense layers. Wide and Deep networks are often effective on tabular datasets.[^1]\n\nBoth the dataset and the model are much larger than what we used in the course. To speed up training, we'll use Kaggle's [Tensor Processing Units](https://www.kaggle.com/docs/tpu) (TPUs), an accelerator ideal for large workloads.\n\nWe've collected some hyperparameters here to make experimentation easier. Fork this notebook by [**clicking here**](https://www.kaggle.com/kernels/fork/12171965) to try it yourself!","metadata":{}},{"cell_type":"code","source":"# Model Configuration\nUNITS = 2 ** 11 # 2048\nACTIVATION = 'relu'\nDROPOUT = 0.1\n\n# Training Configuration\nBATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best","metadata":{"execution":{"iopub.status.busy":"2022-08-10T06:40:49.611932Z","iopub.execute_input":"2022-08-10T06:40:49.612678Z","iopub.status.idle":"2022-08-10T06:40:49.639038Z","shell.execute_reply.started":"2022-08-10T06:40:49.612536Z","shell.execute_reply":"2022-08-10T06:40:49.638145Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"The next few sections set up the TPU computation, data pipeline, and neural network model. If you'd just like to see the results, feel free to skip to the end!\n\n# Setup #\n\nIn addition to our imports, this section contains some code that will connect our notebook to the TPU and create a **distribution strategy**. Each TPU has eight computational cores acting independently. With a distribution strategy, we define how we want to divide up the work between them.","metadata":{}},{"cell_type":"code","source":"# TensorFlow\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\n\n# Detect and init the TPU\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    \n# Plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Matplotlib defaults\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n\n# Data\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.io import FixedLenFeature\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2022-08-10T06:40:49.640578Z","iopub.execute_input":"2022-08-10T06:40:49.641098Z","iopub.status.idle":"2022-08-10T06:41:01.901883Z","shell.execute_reply.started":"2022-08-10T06:40:49.641054Z","shell.execute_reply":"2022-08-10T06:41:01.900837Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Notice that TensorFlow now detects eight accelerators. Using a TPU is a bit like using eight GPUs at once.\n\n# Load Data #\n\nThe dataset has been encoded in a binary file format called *TFRecords*. These two functions will parse the TFRecords and build a TensorFlow `tf.data.Dataset` object that we can use for training.","metadata":{}},{"cell_type":"code","source":"def make_decoder(feature_description):\n    def decoder(example):\n        example = tf.io.parse_single_example(example, feature_description)\n        features = tf.io.parse_tensor(example['features'], tf.float32)\n        features = tf.reshape(features, [28])\n        label = example['label']\n        return features, label\n    return decoder\n\ndef load_dataset(filenames, decoder, ordered=False):\n    AUTO = tf.data.experimental.AUTOTUNE\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = (\n        tf.data\n        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n        .with_options(ignore_order)\n        .map(decoder, AUTO)\n    )\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-08-10T06:41:01.903573Z","iopub.execute_input":"2022-08-10T06:41:01.903816Z","iopub.status.idle":"2022-08-10T06:41:01.911470Z","shell.execute_reply.started":"2022-08-10T06:41:01.903786Z","shell.execute_reply":"2022-08-10T06:41:01.910489Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset_size = int(11e6)\nvalidation_size = int(5e5)\ntraining_size = dataset_size - validation_size\n\n# For model.fit\nbatch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nsteps_per_epoch = training_size // batch_size\nvalidation_steps = validation_size // batch_size\n\n# For model.compile\nsteps_per_execution = 256","metadata":{"execution":{"iopub.status.busy":"2022-08-10T06:41:01.913811Z","iopub.execute_input":"2022-08-10T06:41:01.914219Z","iopub.status.idle":"2022-08-10T06:41:01.922218Z","shell.execute_reply.started":"2022-08-10T06:41:01.914174Z","shell.execute_reply":"2022-08-10T06:41:01.921546Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"feature_description = {\n    'features': FixedLenFeature([], tf.string),\n    'label': FixedLenFeature([], tf.float32),\n}\ndecoder = make_decoder(feature_description)\n\ndata_dir = KaggleDatasets().get_gcs_path('higgs-boson')\ntrain_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\nvalid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n\nds_train = load_dataset(train_files, decoder, ordered=False)\nds_train = (\n    ds_train\n    .cache()\n    .repeat()\n    .shuffle(2 ** 19)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nds_valid = load_dataset(valid_files, decoder, ordered=False)\nds_valid = (\n    ds_valid\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T06:41:01.923772Z","iopub.execute_input":"2022-08-10T06:41:01.924091Z","iopub.status.idle":"2022-08-10T06:41:02.594201Z","shell.execute_reply.started":"2022-08-10T06:41:01.924053Z","shell.execute_reply":"2022-08-10T06:41:02.593444Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model #\n\nNow that the data is ready, let's define the network. We're defining the deep branch of the network using Keras's *Functional API*, which is a bit more flexible that the `Sequential` method we used in the course.\n","metadata":{}},{"cell_type":"code","source":"def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        return x\n    return make\n\nwith strategy.scope():\n    # Wide Network\n    wide = keras.experimental.LinearModel()\n\n    # Deep Network\n    inputs = keras.Input(shape=[28])\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    outputs = layers.Dense(1)(x)\n    deep = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Wide and Deep Network\n    wide_and_deep = keras.experimental.WideDeepModel(\n        linear_model=wide,\n        dnn_model=deep,\n        activation='sigmoid',\n    )\n\nwide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n    experimental_steps_per_execution=steps_per_execution,\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T06:41:02.595468Z","iopub.execute_input":"2022-08-10T06:41:02.595726Z","iopub.status.idle":"2022-08-10T06:41:03.526646Z","shell.execute_reply.started":"2022-08-10T06:41:02.595695Z","shell.execute_reply":"2022-08-10T06:41:03.525857Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Training #\n\nDuring training, we'll use the `EarlyStopping` callback as usual. Notice that we've also defined a **learning rate schedule**. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by `0.2` if the validation loss didn't decrease after an epoch.","metadata":{}},{"cell_type":"code","source":"early_stopping = callbacks.EarlyStopping(\n    patience=2,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001,\n)","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2022-08-10T06:41:03.528088Z","iopub.execute_input":"2022-08-10T06:41:03.528621Z","iopub.status.idle":"2022-08-10T06:41:03.534351Z","shell.execute_reply.started":"2022-08-10T06:41:03.528569Z","shell.execute_reply":"2022-08-10T06:41:03.533338Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"history = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n    callbacks=[early_stopping, lr_schedule],\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T06:41:03.535905Z","iopub.execute_input":"2022-08-10T06:41:03.536198Z","iopub.status.idle":"2022-08-10T06:51:46.406798Z","shell.execute_reply.started":"2022-08-10T06:41:03.536167Z","shell.execute_reply":"2022-08-10T06:51:46.405944Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","metadata":{"execution":{"iopub.status.busy":"2022-08-10T06:51:46.410705Z","iopub.execute_input":"2022-08-10T06:51:46.411001Z","iopub.status.idle":"2022-08-10T06:51:47.227010Z","shell.execute_reply.started":"2022-08-10T06:51:46.410968Z","shell.execute_reply":"2022-08-10T06:51:47.226348Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# References #\n\n- Baldi, P. et al. *Searching for Exotic Particles in High-Energy Physics with Deep Learning*. (2014) ([arXiv](https://arxiv.org/abs/1402.4735))\n- Cheng, H. et al. *Wide & Deep Learning for Recommender Systems*. (2016) ([arXiv](https://arxiv.org/abs/1606.07792))\n- *What Exactly is the Higgs Boson?* Scientific American. (1999) [(article)](https://www.scientificamerican.com/article/what-exactly-is-the-higgs/)]\n\n[^1]: In the original implementation, categorical features were one-hot encoded and crossed to produce the interaction features. This \"wide\" dataset was used with the linear component. For the deep component, the categories were encoded into a much narrower embedding layer.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/191966) to chat with other Learners.*","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}