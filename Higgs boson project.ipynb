{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Searching for the Higgs Boson #\n","\n","Large and complicated datasets like these are where deep learning excels. In this notebook, we'll build a Wide and Deep neural network to determine whether an observed particle collision produced a Higgs boson or not.\n","\n","# The Collision Data #\n","\n","The collision of protons at high energy can produce new particles like the Higgs boson. These particles can't be directly observed, however, since they decay almost instantly. So to detect the presence of a new particle, we instead observe the behavior of the particles they decay into, their \"decay products\".\n","\n","The *Higgs* dataset contains 21 \"low-level\" features of the decay products and also 7 more \"high-level\" features derived from these.\n","\n","# Wide and Deep Neural Networks #\n","\n","A *Wide and Deep* network trains a linear layer side-by-side with a deep stack of dense layers. Wide and Deep networks are often effective on tabular datasets.[^1]\n","\n","Both the dataset and the model are much larger than what we used in the course. To speed up training, we'll use Kaggle's [Tensor Processing Units](https://www.kaggle.com/docs/tpu) (TPUs), an accelerator ideal for large workloads.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:40:49.612678Z","iopub.status.busy":"2022-08-10T06:40:49.611932Z","iopub.status.idle":"2022-08-10T06:40:49.639038Z","shell.execute_reply":"2022-08-10T06:40:49.638145Z","shell.execute_reply.started":"2022-08-10T06:40:49.612536Z"},"trusted":true},"outputs":[],"source":["# Model Configuration\n","UNITS = 2 ** 11 # 2048\n","ACTIVATION = 'relu'\n","DROPOUT = 0.1\n","\n","# Training Configuration\n","BATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best"]},{"cell_type":"markdown","metadata":{},"source":["The next few sections set up the TPU computation, data pipeline, and neural network model. If you'd just like to see the results, feel free to skip to the end!\n","\n","# Setup #\n","\n","In addition to our imports, this section contains some code that will connect our notebook to the TPU and create a **distribution strategy**. Each TPU has eight computational cores acting independently. With a distribution strategy, we define how we want to divide up the work between them."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:40:49.641098Z","iopub.status.busy":"2022-08-10T06:40:49.640578Z","iopub.status.idle":"2022-08-10T06:41:01.901883Z","shell.execute_reply":"2022-08-10T06:41:01.900837Z","shell.execute_reply.started":"2022-08-10T06:40:49.641054Z"},"trusted":true},"outputs":[],"source":["# TensorFlow\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","# Detect and init the TPU\n","try: # detect TPUs\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","except ValueError: # detect GPUs\n","    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n","print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n","    \n","# Plotting\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Matplotlib defaults\n","plt.style.use('seaborn-whitegrid')\n","plt.rc('figure', autolayout=True)\n","plt.rc('axes', labelweight='bold', labelsize='large',\n","       titleweight='bold', titlesize=18, titlepad=10)\n","\n","\n","# Data\n","from kaggle_datasets import KaggleDatasets\n","from tensorflow.io import FixedLenFeature\n","AUTO = tf.data.experimental.AUTOTUNE\n","\n","\n","# Model\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import callbacks"]},{"cell_type":"markdown","metadata":{},"source":["Notice that TensorFlow now detects eight accelerators. Using a TPU is a bit like using eight GPUs at once.\n","\n","# Load Data #\n","\n","The dataset has been encoded in a binary file format called *TFRecords*. These two functions will parse the TFRecords and build a TensorFlow `tf.data.Dataset` object that we can use for training."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:41:01.903816Z","iopub.status.busy":"2022-08-10T06:41:01.903573Z","iopub.status.idle":"2022-08-10T06:41:01.911470Z","shell.execute_reply":"2022-08-10T06:41:01.910489Z","shell.execute_reply.started":"2022-08-10T06:41:01.903786Z"},"trusted":true},"outputs":[],"source":["def make_decoder(feature_description):\n","    def decoder(example):\n","        example = tf.io.parse_single_example(example, feature_description)\n","        features = tf.io.parse_tensor(example['features'], tf.float32)\n","        features = tf.reshape(features, [28])\n","        label = example['label']\n","        return features, label\n","    return decoder\n","\n","def load_dataset(filenames, decoder, ordered=False):\n","    AUTO = tf.data.experimental.AUTOTUNE\n","    ignore_order = tf.data.Options()\n","    if not ordered:\n","        ignore_order.experimental_deterministic = False\n","    dataset = (\n","        tf.data\n","        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n","        .with_options(ignore_order)\n","        .map(decoder, AUTO)\n","    )\n","    return dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:41:01.914219Z","iopub.status.busy":"2022-08-10T06:41:01.913811Z","iopub.status.idle":"2022-08-10T06:41:01.922218Z","shell.execute_reply":"2022-08-10T06:41:01.921546Z","shell.execute_reply.started":"2022-08-10T06:41:01.914174Z"},"trusted":true},"outputs":[],"source":["dataset_size = int(11e6)\n","validation_size = int(5e5)\n","training_size = dataset_size - validation_size\n","\n","# For model.fit\n","batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","steps_per_epoch = training_size // batch_size\n","validation_steps = validation_size // batch_size\n","\n","# For model.compile\n","steps_per_execution = 256"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:41:01.924091Z","iopub.status.busy":"2022-08-10T06:41:01.923772Z","iopub.status.idle":"2022-08-10T06:41:02.594201Z","shell.execute_reply":"2022-08-10T06:41:02.593444Z","shell.execute_reply.started":"2022-08-10T06:41:01.924053Z"},"trusted":true},"outputs":[],"source":["feature_description = {\n","    'features': FixedLenFeature([], tf.string),\n","    'label': FixedLenFeature([], tf.float32),\n","}\n","decoder = make_decoder(feature_description)\n","\n","data_dir = KaggleDatasets().get_gcs_path('higgs-boson')\n","train_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\n","valid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n","\n","ds_train = load_dataset(train_files, decoder, ordered=False)\n","ds_train = (\n","    ds_train\n","    .cache()\n","    .repeat()\n","    .shuffle(2 ** 19)\n","    .batch(batch_size)\n","    .prefetch(AUTO)\n",")\n","\n","ds_valid = load_dataset(valid_files, decoder, ordered=False)\n","ds_valid = (\n","    ds_valid\n","    .batch(batch_size)\n","    .cache()\n","    .prefetch(AUTO)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Model #\n","\n","Now that the data is ready, let's define the network. We're defining the deep branch of the network using Keras's *Functional API*, which is a bit more flexible that the `Sequential` method we used in the course.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:41:02.595726Z","iopub.status.busy":"2022-08-10T06:41:02.595468Z","iopub.status.idle":"2022-08-10T06:41:03.526646Z","shell.execute_reply":"2022-08-10T06:41:03.525857Z","shell.execute_reply.started":"2022-08-10T06:41:02.595695Z"},"trusted":true},"outputs":[],"source":["def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n","    def make(inputs):\n","        x = layers.Dense(units)(inputs)\n","        x = layers.BatchNormalization()(x)\n","        x = layers.Activation(activation)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","        return x\n","    return make\n","\n","with strategy.scope():\n","    # Wide Network\n","    wide = keras.experimental.LinearModel()\n","\n","    # Deep Network\n","    inputs = keras.Input(shape=[28])\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n","    outputs = layers.Dense(1)(x)\n","    deep = keras.Model(inputs=inputs, outputs=outputs)\n","    \n","    # Wide and Deep Network\n","    wide_and_deep = keras.experimental.WideDeepModel(\n","        linear_model=wide,\n","        dnn_model=deep,\n","        activation='sigmoid',\n","    )\n","\n","wide_and_deep.compile(\n","    loss='binary_crossentropy',\n","    optimizer='adam',\n","    metrics=['AUC', 'binary_accuracy'],\n","    experimental_steps_per_execution=steps_per_execution,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Training #\n","\n","During training, we'll use the `EarlyStopping` callback as usual. Notice that we've also defined a **learning rate schedule**. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by `0.2` if the validation loss didn't decrease after an epoch."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:41:03.528621Z","iopub.status.busy":"2022-08-10T06:41:03.528088Z","iopub.status.idle":"2022-08-10T06:41:03.534351Z","shell.execute_reply":"2022-08-10T06:41:03.533338Z","shell.execute_reply.started":"2022-08-10T06:41:03.528569Z"},"lines_to_next_cell":2,"trusted":true},"outputs":[],"source":["early_stopping = callbacks.EarlyStopping(\n","    patience=2,\n","    min_delta=0.001,\n","    restore_best_weights=True,\n",")\n","\n","lr_schedule = callbacks.ReduceLROnPlateau(\n","    patience=0,\n","    factor=0.2,\n","    min_lr=0.001,\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:41:03.536198Z","iopub.status.busy":"2022-08-10T06:41:03.535905Z","iopub.status.idle":"2022-08-10T06:51:46.406798Z","shell.execute_reply":"2022-08-10T06:51:46.405944Z","shell.execute_reply.started":"2022-08-10T06:41:03.536167Z"},"trusted":true},"outputs":[],"source":["history = wide_and_deep.fit(\n","    ds_train,\n","    validation_data=ds_valid,\n","    epochs=50,\n","    steps_per_epoch=steps_per_epoch,\n","    validation_steps=validation_steps,\n","    callbacks=[early_stopping, lr_schedule],\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-10T06:51:46.411001Z","iopub.status.busy":"2022-08-10T06:51:46.410705Z","iopub.status.idle":"2022-08-10T06:51:47.227010Z","shell.execute_reply":"2022-08-10T06:51:47.226348Z","shell.execute_reply.started":"2022-08-10T06:51:46.410968Z"},"trusted":true},"outputs":[],"source":["history_frame = pd.DataFrame(history.history)\n","history_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\n","history_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
